# -*- coding: utf-8 -*-
"""Tweet_generation_gpt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qlo2RSttadZZhn9iSOy3KQQxwyEw3j7T

# Tweet Generation by fine tuning gpt-2

### Author - Devendra Kayande

## Installing and loading dependencies
"""

!pip install transformers # installing hugging face library

!pip install datasets

!pip install --upgrade accelerate

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import random
import transformers
import re
import string
import gensim
from string import punctuation
from wordcloud import WordCloud
import os, sys, re, uuid, time, warnings,sklearn,nltk, logging, functools, time
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from transformers import AutoTokenizer
from collections import Counter

from transformers import GPT2Tokenizer
from datasets import Dataset
from transformers import AutoTokenizer, GPT2LMHeadModel
from transformers.optimization import Adafactor 
from transformers.optimization import get_scheduler
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorForLanguageModeling
from datasets import list_metrics
from datasets import load_metric

transformers.__version__

"""## Loading  tweets

I already downloaded sentiment analysis data from Kaggle and uploaded to my GDrive and loading the data from Drive.
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/Twitter_Sentiment_Analysis.zip

csv_file='/content/training.1600000.processed.noemoticon.csv'
columns = ['target','ids','date','flag','user','text']
df=pd.read_csv(csv_file,names=columns,encoding='ISO-8859-1')
df.head()

tweets=df[['text']].astype(str)
tweets

tweets.isnull().sum()

df_1=tweets.iloc[:40000]
df_1.to_csv('/content/drive/MyDrive/chunk_1.csv')

df_2=tweets.iloc[40000:80000]
df_2.to_csv('/content/drive/MyDrive/chunk_2.csv')

df_3=tweets.iloc[80000:120000]
df_3.to_csv('/content/drive/MyDrive/chunk_3.csv')

df_4=tweets.iloc[120000:160000]
df_4.to_csv('/content/drive/MyDrive/chunk_4.csv')

tweets_list=list(df.text)

"""## Text Preprocessing"""

nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')

"""Making a stopword list"""

stopwordlist = set(stopwords.words('english'))- {'not', 'no', 'never'}
stopwordlist

"""Making a function for preprocessing which includes removing stopwords,urls,tags,alphanumerics and stemming"""

def preprocess(textdata):
    processedText =[]



    # Defining regex patterns.
    urlPattern = r"((http://)[^ ]*|(https://)[^ ]*|( www\.)[^ ]*)"
    userPattern = '@[^\s]+'
    alphaPattern = "[^a-zA-Z0-9]\s"
    sequencePattern = r"(.)\1\1+"
    seqReplacePattern = r"\1\1"

    for tweet in textdata:
        tweet = tweet.lower()

        # Replace all URls with nothing
        tweet = re.sub(urlPattern, "", tweet,flags=re.MULTILINE)
        # Replace @USERNAME to nothing.
        tweet = re.sub(userPattern, "", tweet)
        # Replace all non alphabets.
        tweet = re.sub(alphaPattern, "", tweet)
        # Replace 3 or more consecutive letters by 2 letter.
        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)
        # Remove punctuation
        tweet = tweet.translate(str.maketrans("","",string.punctuation))
            
            
        # Remove Stopwords
        tweetTokens = word_tokenize(tweet)
        filteredWords = [word for word in tweetTokens if word not in stopwordlist]
        
        # stemming 
        ps = PorterStemmer()
        stemmedWords = [ps.stem(word) for word in filteredWords]
        
        # Create Lemmatizer.
        wordLemm = WordNetLemmatizer()   
        lemmaWords = [wordLemm.lemmatize(word,pos='a') for word in stemmedWords]
        
        # Joining the words of a tweet 
        cleanTweet = " ".join(lemmaWords)
        
        # Adding the tweet to the processedText array
        processedText.append(cleanTweet)

        # Converting again to dataframe
        df=pd.DataFrame(processedText,columns=['preprocessed_text'])

    return processedText,df

tweets_preprocessed_list,tweets_preprocessed_df = preprocess(tweets_list)

"""Loading our all chunks of data"""

pdf_1=pd.read_csv('/content/drive/MyDrive/pchunk_1.csv')
pdf_2=pd.read_csv('/content/drive/MyDrive/pchunk_2.csv')
pdf_3=pd.read_csv('/content/drive/MyDrive/pchunk_3.csv')
pdf_4=pd.read_csv('/content/drive/MyDrive/pchunk_4.csv')

tweets_preprocessed_df=pd.concat([pdf_1,pdf_2,pdf_3,pdf_4])

tweets_preprocessed_df.isnull().sum()

tweets_preprocessed_df.dropna(inplace=True)

tweets_preprocessed_df=tweets_preprocessed_df[['preprocessed_text']].astype(str)

tweets_preprocessed_df

"""Saving our preprocessed csv file """

tweets_preprocessed_df.to_csv('/content/drive/MyDrive/tweets.csv')
tweets_preprocessed_df.to_csv('/content/drive/MyDrive/tweets_no_headers.csv',header=False, index=False)

"""Checking distribution of lengths of tweets"""

doc_lengths = []

for elem in tweets_preprocessed_df['preprocessed_text']:
    # get rough token count distribution 
    tokens = nltk.word_tokenize(elem)
    doc_lengths.append(len(tokens))

doc_lengths = np.array(doc_lengths)
sns.displot(doc_lengths)
print(np.average(doc_lengths))

"""Just for visualizing most frequent words in the corpus"""

#generate a word cloud image of tweets
text = " ".join(tweet for tweet in tweets_preprocessed_df['preprocessed_text'])
wordcloud = WordCloud(width = 1600, height = 800, max_words=100, background_color="white").generate(text)

#display the generated image
plt.figure( figsize=(20,10) )
plt.imshow(wordcloud)
plt.axis("off")

"""<pre> <i>We will be using GPT-2 and fine tuning it using our tweet data hence we need to do tokenizing and preprocessing according to the process of its original pretraining.</i> </pre>

Tokenizing the data
"""

# Load the GPT tokenizer.
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', 
                                          bos_token='<|startoftext|>', 
                                          eos_token='<|endoftext|>', 
                                          pad_token='<|pad|>')

"""<pre>The beginning of sequence token:
bos_token =  <|startoftext|> 

This token tells our model from which start generating words for text/tweet.

The end of sequence token:
eos_token = <|endoftext|>

This token tells our model when to stop generating words for text/tweet.

The padding token:
pad_token= <|pad|>

This token is use to complete the length of a particular tweet, i.e if we have a tweet with length 30 however we use as maximum length 50, the tokenizer will add pad_token just to complete the length of 50, however the model not will give attention to this padding token.
</pre>
"""

training_examples = f'<|startoftext|> ' + tweets_preprocessed_df['preprocessed_text'] + '<|endoftext|>'

print(training_examples[0])

"""Creating a new DataFrame for these tokenized tweets"""

tweet_tokenized_df = pd.DataFrame({'text': training_examples})

tweet_tokenized_df.head(5)

"""## Creating a Hugging Face Dataset

<pre> By creating a Hugging Face Dataset we can easily fine tune our Hugging Face Transformers. </pre>
"""

tweet_data = Dataset.from_pandas(tweet_tokenized_df)  # turn a pandas DataFrame into a Hugging Face Dataset

def preprocess(example):  # tokenize our text but don't pad because our collator will pad for us dynamically
    return tokenizer(example['text'], truncation=True)

"""Splitting data"""

tweet_data = tweet_data.map(preprocess, batched=False)

tweet_data = tweet_data.train_test_split(train_size=.8)

"""Initializing Hugging Face Data Collator so that we can do tokenizing and padding dynamically for our batches."""

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

"""## Building and training GPT-2 model

We are using pretrained model and will fine tune it.
"""

model = GPT2LMHeadModel.from_pretrained('gpt2')
model.resize_token_embeddings(len(tokenizer))

device = torch.device("cuda")
model.to(device)

"""Training"""

epochs=3
batch_size=64
learning_rate=1e-5
warmup_steps=500

optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)
lr_scheduler = get_scheduler(optimizer=optimizer,name='constant')
# lrn_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
#     optimizer, patience=10, verbose=True)

print(optimizer.param_groups[0]['lr'])

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/tweets",
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs= epochs, # number of training epochs
    per_device_train_batch_size=batch_size, # batch size for training
    per_device_eval_batch_size=batch_size,  # batch size for evaluation
    load_best_model_at_end=True,
    logging_steps=5,
    log_level='info',
    evaluation_strategy='epoch',
    save_strategy='epoch',
    learning_rate= learning_rate,
    warmup_steps=warmup_steps,
    seed= 38,
    
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tweet_data["train"],
    eval_dataset=tweet_data["test"],
    data_collator=data_collator,
    optimizers = (optimizer, lr_scheduler),
)

# train the model
trainer.train()

"""## Generating Tweets

<pre>We will give seed sentence in the form of a prompt.</pre>
"""

seed_sentence = input('Enter the seed sentence: ')
print(f'You entered {seed_sentence}')

model.eval()

prompt = "<|startoftext|>" + seed_sentence

generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)
generated = generated.to(device)

print(generated)

sample_outputs = model.generate(
                                generated,
                                do_sample=True,   
                                top_k=20, 
                                max_length = 300,
                                top_p=0.98, 
                                num_return_sequences=10,
                                )

for i, sample_output in enumerate(sample_outputs):
  print("{}: {}\n\n".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))

"""## Evaluating the model on GLEU benchmark"""

sample_outputs[0]

from datasets import load_metric
metric = load_metric('glue','stsb')

print(metric.inputs_description)

true_ref = "<|startoftext|>" + "hello how are you working so much late"

gold_ref = torch.tensor(tokenizer.encode(true_ref)).unsqueeze(0)
gold_ref = gold_ref.to(device)
gold_ref=gold_ref.squeeze().tolist()

print(gold_ref)

model_predictions = sample_outputs[1].tolist()
model_predictionsn=model_predictions[:9]
model_predictionsn

final_score = metric.compute(predictions=model_predictionsn, references=gold_ref)
final_score

"""# Conclusion and some more steps

<pre>
- The fine tuning process can be made more efficient to reduce losses by data cleaning, implementing learning rate scheduling and other hyperparameter tuning.
- For generating proper tweet structure , we need to modify our text pre processing steps like for example stopwords are extensively used in tweets and characters like '@' and '#' are also used massively which we removed in the preprocessing as mentioned to  do in task.
</pre>
"""